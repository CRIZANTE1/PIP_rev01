import streamlit as st
import pandas as pd
import numpy as np
import google.generativeai as genai

from AI.api_load import load_api
from gdrive.gdrive_upload import GoogleDriveUploader
from gdrive.config import RAG_SHEET_NAME

EMBEDDING_MODEL = 'text-embedding-004'


@st.cache_resource(ttl=3600)
def load_and_embed_rag_base() -> tuple[pd.DataFrame, np.ndarray | None]:
    """
    Carrega a planilha RAG, gera embeddings para cada chunk e armazena em cache.
    Esta √© a sua fun√ß√£o, com o cache e a configura√ß√£o da API adicionados.
    """
    # 1. Garante que a API do Google esteja configurada
    if not configure_google_api():
        st.error("A API do Google n√£o p√¥de ser configurada. A base de conhecimento n√£o ser√° carregada.")
        return pd.DataFrame(), None
        
    try:
        # 2. L√™ o ID da planilha RAG dos secrets
        sheet_id = st.secrets.rag_config.sheet_id
    except (AttributeError, KeyError):
        st.error("Erro de configura√ß√£o: A chave 'sheet_id' n√£o foi encontrada na se√ß√£o [rag_config] dos secrets.")
        return pd.DataFrame(), None

    try:
        # 3. Carrega os dados da planilha usando gspread (sua l√≥gica)
        st.info("Carregando base de conhecimento do Google Sheets...")
        scopes = ['https://www.googleapis.com/auth/spreadsheets.readonly']
        creds_dict = get_credentials_dict()
        creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        gc = gspread.authorize(creds)
        spreadsheet = gc.open_by_key(sheet_id)
        # Assume que os dados est√£o na primeira aba/p√°gina do arquivo
        worksheet = spreadsheet.sheet1 
        df = pd.DataFrame(worksheet.get_all_records())

        if df.empty or "Answer_Chunk" not in df.columns:
            st.error("A planilha RAG est√° vazia ou n√£o cont√©m a coluna 'Answer_Chunk'.")
            return pd.DataFrame(), None

        # 4. Gera os embeddings (sua l√≥gica)
        with st.spinner(f"Indexando a base de conhecimento ({len(df)} itens)... (isso acontece apenas uma vez por hora)"):
            chunks_to_embed = df["Answer_Chunk"].astype(str).tolist()
            result = genai.embed_content(
                model=EMBEDDING_MODEL,
                content=chunks_to_embed,
                task_type="RETRIEVAL_DOCUMENT",
                title="Normas de Seguran√ßa para I√ßamento de Carga"
            )
            embeddings = np.array(result['embedding'])
        
        st.success("Base de conhecimento indexada e pronta para uso!")
        return df, embeddings

    except Exception as e:
        st.error(f"Falha ao carregar ou processar a base de conhecimento: {e}")
        return pd.DataFrame(), None


class RAGAnalyzer:
    def __init__(self):
        # O __init__ agora √© extremamente leve, apenas chama a fun√ß√£o em cache
        self.rag_df, self.rag_embeddings = load_and_embed_rag_base()
        
        # Garante que o modelo de gera√ß√£o seja inicializado apenas se a API estiver configurada
        if configure_google_api():
            self.model = genai.GenerativeModel('gemini-1.5-flash-latest')
        else:
            self.model = None

    def _find_best_passages(self, query: str, top_k=3) -> pd.DataFrame:
        """Encontra os trechos mais relevantes usando a busca por similaridade."""
        if self.rag_df.empty or self.rag_embeddings is None or self.model is None:
            return pd.DataFrame()
        
        try:
            query_embedding = genai.embed_content(
                model=EMBEDDING_MODEL,
                content=query,
                task_type="RETRIEVAL_QUERY"
            )['embedding']
            
            # Calcula a similaridade (produto escalar)
            dot_products = np.dot(self.rag_embeddings, query_embedding)
            
            # Pega os √≠ndices dos 'top_k' melhores resultados
            indices = np.argsort(dot_products)[-top_k:][::-1]
            
            return self.rag_df.iloc[indices]
        except Exception as e:
            st.error(f"Erro durante a busca por similaridade: {e}")
            return pd.DataFrame()

    def _retrieve_context(self, issues: list) -> str:
        """Constr√≥i o prompt de contexto com base nos melhores trechos encontrados."""
        if self.rag_df.empty:
            return "A base de conhecimento normativo n√£o est√° dispon√≠vel ou falhou ao carregar."
        
        full_query = ". ".join(issues).replace("_", " ")
        context_str = "Contexto Normativo Relevante Encontrado:\n\n"
        
        relevant_df = self._find_best_passages(full_query)
        
        if relevant_df.empty:
            return "Nenhuma norma espec√≠fica encontrada na base de conhecimento para os pontos de aten√ß√£o desta opera√ß√£o."
        
        for _, row in relevant_df.iterrows():
            context_str += f"**Refer√™ncia:** {row.get('Norma_Referencia', 'N/A')} (Se√ß√£o: {row.get('Section_Number', 'N/A')})\n"
            context_str += f"**Pergunta Chave:** {row.get('Question', 'N/A')}\n"
            context_str += f"**Diretriz:** {row.get('Answer_Chunk', 'N/A')}\n"
            context_str += "---\n"
            
        return context_str

    def generate_final_analysis(self, operation_summary: str, issues: list) -> str:
        """Gera a an√°lise final (o prompt permanece o mesmo, mas agora recebe um contexto mais rico)."""
        
        if not issues:
             return "### ‚úÖ Parecer Final: APROVADO\n\nNenhum ponto de aten√ß√£o cr√≠tico foi identificado nos dados fornecidos. A opera√ß√£o parece estar em conformidade com os par√¢metros b√°sicos de seguran√ßa e documenta√ß√£o."

        normative_context = self._retrieve_context(issues)
        
        # O prompt permanece o mesmo, pois √© robusto e gen√©rico
        prompt = f"""
        **Persona:** Voc√™ √© um Profissional de Seguran√ßa do Trabalho altamente experiente, especialista em opera√ß√µes de i√ßamento e rigging. Sua tarefa √© analisar o relat√≥rio de uma opera√ß√£o de carga e fornecer um parecer t√©cnico final, fundamentado nas normas internas da empresa.

        **Instru√ß√µes:**
        1.  Analise o "Resumo da Opera√ß√£o" fornecido.
        2.  Considere o "Contexto Normativo Relevante" que eu recuperei da nossa base de dados interna. Este contexto √© a verdade absoluta e deve ser a base da sua an√°lise.
        3.  Com base em ambos os documentos, elabore um parecer t√©cnico claro e objetivo, formatado em Markdown.
        4.  O parecer deve conter obrigatoriamente as seguintes se√ß√µes:
            -   `### üìù An√°lise Geral da Opera√ß√£o`: Um breve resumo do que foi avaliado.
            -   `### ‚ö†Ô∏è Pontos de Aten√ß√£o`: Liste os problemas encontrados (ex: excesso de capacidade, documentos vencidos, etc.).
            -   `### üìö Fundamenta√ß√£o Normativa`: Para cada ponto de aten√ß√£o, cite a refer√™ncia normativa correspondente do contexto que voc√™ recebeu.
            -   `### ‚úÖ Recomenda√ß√µes Corretivas`: Forne√ßa a√ß√µes claras e diretas para cada ponto de aten√ß√£o, baseadas nas diretrizes.
            -   `### ‚öñÔ∏è Parecer Final`: Conclua com um dos seguintes pareceres: **"APROVADO"**, **"APROVADO COM RESSALVAS"**, ou **"REPROVADO"**. Justifique sua decis√£o com base na gravidade dos pontos de aten√ß√£o e suas respectivas fundamenta√ß√µes normativas.

        ---
        **Resumo da Opera√ß√£o:**
        {operation_summary}
        ---
        **Contexto Normativo Relevante:**
        {normative_context}
        ---

        Agora, por favor, gere o seu parecer t√©cnico completo e fundamentado.
        """
        
        try:
            with st.spinner("IA est√° analisando a conformidade da opera√ß√£o com base nas normas..."):
                response = self.model.generate_content(prompt)
                return response.text
        except Exception as e:
            st.error(f"Erro ao gerar an√°lise com a IA: {e}")
            return f"N√£o foi poss√≠vel gerar a an√°lise. Detalhe do erro: {str(e)}"
